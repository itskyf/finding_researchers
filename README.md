# Introduction

This document is divided into two main sections:

1. A *naive* solution that I could implemented within the given time frame.
2. Further research.

# Basic solution

## Concept

Thanks to the [scholarly](https://github.com/scholarly-python-package/scholarly) package, I discovered that Google Scholar supports author searches based on their respective areas of interest using the query parameter *mauthors=label:computer_vision*. As the API results appear to be sorted by citation count, I devised a heuristic that prioritizes first authors, presuming they are the foremost experts based on their high citation counts.

The proposed initial solutions are as follows:

1. Extract publication categories from the [top venues](https://scholar.google.com/citations?view_op=top_venues) site through web crawling.
2. Identify categories that closely align with the input query, using sentence similarity.
3. Utilize the mauthors API parameter to seek authors specializing in the identified research categories.

However, I suspect there might exist more conventional methods to identify the categories.

## Technical

### Installation
Use conda environment file to install the environment
```bash
conda env create --file sentence_transformers.yaml
```

### Pre-compute categories embbeding
Execute the code in Stuff.ipynb, and it will generate a new directory, ./data/google_scholar/, containing two JSON files that outline research categories. Additionally, a binary file will be created to store sentence embeddings associated with each category.

### Run the query
```bash
python naive_run.py "{research domain query}"
```

## Limitation
- Currently, this method does not support the processing of additional criteria.
- The Google Scholar API lacks documentation about the returned results.

# Further research
Since I am new to this problem, I spent a considerable amount of time gathering insights. Although still struggling on arranging those piece of information, I propose that a more effective strategy involves constructing a knowledge graph incorporating authors and their respective research domains, and subsequently integrating this with criteria extracted using NLP techniques.

## Data
### Scholar Semantic
They provide a large quantity of papers in various domains. Details about their data can be found in ["The Semantic Scholar Open Data Platform"](https://www.semanticscholar.org/paper/The-Semantic-Scholar-Open-Data-Platform-Kinney-Anastasiades/cb92a7f9d9dbcf9145e32fdfa0e70e2a6b828eb1), which provides insights into their services and data processing pipeline. During my research, I found two pieces of information:
- Graph API: this allows for the retrieval of paper and author data. Each paper is accompanied by a vector embedding. While it includes a 'fieldsOfStudy' field generated by an SVM classification model, the information provided is limited to a general level 0 taxonomy.
- Dataset API: to download the full-corpus datasets. **It includes Incremental Diffs**, which can be used to keep the data up to date. There is **./data/S2AG/fetch_samples.bash** script to download dataset samples.
### arXiv
On [Kaggle](https://www.kaggle.com/datasets/Cornell-University/arxiv), they publish a weekly updated dataset, which each paper record has a *categories* field following their classification scheme. That field is specific research domains, so in my opinion **this dataset can be leveraged to create labels for a categorical classification** model for the new data from the diff of Scholar Semantic.

## NLP
I have read about NLP tasks like Named Entity Recognition, or Open Information Extraction; and some language model trained on scientific text (OAG-BERT, SciBERT, etc.), but not clear enough how to use them to extract usable data from input criteria.

